---
title: "Evaluate testing data (multi-class) - Lasso"
author: "Andrew Chang"
date: '`r Sys.Date()`'
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
---

Note: The two differences between Lasso and Tree-based methods are:

1. Lasso has its own inherent feature selection process.
2. Lasso's vimp will be based on how many times the feature exist in all runs. Regression coefficients may be presented for binary outcomes

 



```{r}
## user input
project_home <- "~/EVE/tests"
project_name <- "lasso_multi_outCV_test"
```

## 0. Load Data
```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
library(broom)
library(caret)
library(pROC)
source("~/EVE/eve/reports/utils/PerformanceUtils.R")
load(paste0(project_home,"/log/", project_name, "/metainfo.Rdata"))

if(grepl("\\.r$|\\.R$", runSpec$engine)){
  df.preval <- getResults(project_home, project_name, "rdata", objName = "df_pred")
  df.vimp   <- getResults(project_home, project_name, "rdata", objName = "df_vimp")
} else {
  df.preval <- getResults(project_home, project_name, "prevalidation")
  df.vimp   <- getResults(project_home, project_name, "vimp")
}

## get original data info
df.orig <- read_csv(paste(project_home, runSpec$training_data, sep="/"))

tot.sample <- dim(df.orig)[1]
tot.feature <- max(df.preval$size)
num.cv <- length(unique(df.preval$cv))
num.seed <- length(unique(df.preval$seed))

cat(tot.sample, "of samples were used \n")
cat(tot.feature, "of full features\n")
cat(num.seed, "runs, each run contains", num.cv, "CVs.\n")
cat("Labels:\n")
table(df.orig[[runSpec$label_name]])

if( !is.na(runSpec$weight_col) && nchar(runSpec$weight_col) && runSpec$weight_col %in% colnames(df.orig)){
  print('summary of weigths')
  if(runSpec$family %in% c("multinomial", "binomial")){
    table(df.orig[[runSpec$label_name]], df.orig[[runSpec$weight_col]], useNA='ifany')
  }else{
    summary(df.orig[[runSpec$weight_col]])
  }
}

```


## 1. Scores
 

Note: '# of feature retain' is not meaningful in this setup. It is just the total number of features. 
The RFE concept is cooked into 'lambda' optimization, 
and each CV may have different lambda values, 
so it makes comparing prevalidation scores at different lambda difficult.

```{r, fig.width=6, fig.height=6, echo=FALSE, message=FALSE, warning=FALSE}
plt <- plotScores(df.preval, runSpec$label_name)
plt$byClass +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_y_continuous(breaks = seq(0, 1, 0.1))
```

```{r, echo=FALSE}
plt$overall +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



## 2. Important Features

For Lasso, we calculate how many times a given features is being used in all the runs.

```{r, message=FALSE, echo=FALSE, warning=FALSE}
main <- paste('distribution across', length(unique(df.vimp$seed)), 'seed x', length(unique(df.vimp$cv)),'CV')
xlab <- paste('# of times a feature is selected by lasso (alpha=', runSpec$alpha,')')
hist(table(df.vimp$feature), main = main , xlab = xlab)
```

```{r, fig.height=6, echo=FALSE}
par(mar=c(5,20,4,2))
barplot(tail(sort(table(df.vimp$feature)), 20), 
        horiz =T, las=2, main='most used features')
```





