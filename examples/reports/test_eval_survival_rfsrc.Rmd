---
title: "Evaluate testing data (survival) - rfsrc"
author: "EVE W."
date: '`r Sys.Date()`'
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
---

Label: os_time

```{r}
## user input
project_home <- "~/EVE/examples"
project_name <- "rfsrc_outCV_test"
```


## 0. Load Data

```{r, message=FALSE, comment=NA, echo=FALSE, warning=FALSE}
library(tidyverse)
library(survival)
library(survminer)
library(randomForestSRC)
library(broom)
source("~/EVE/eve/reports/utils/PerformanceUtils.R")
load(paste0(project_home,"/log/", project_name, "/metainfo.Rdata"))

if(grepl("\\.r$|\\.R$", runSpec$engine)){
  df.preval <- getResults(project_home, project_name, "rdata", objName = "df_pred")
  df.vimp   <- getResults(project_home, project_name, "rdata", objName = "df_vimp")
} else {
  df.preval <- getResults(project_home, project_name, "prevalidation")
  df.vimp   <- getResults(project_home, project_name, "vimp")
}

## get original data info
df.orig <- read_csv(paste(project_home, runSpec$training_data, sep="/"))

tot.sample <- dim(df.orig)[1]
tot.feature <- max(df.preval$size)
num.cv <- length(unique(df.preval$cv))
num.seed <- length(unique(df.preval$seed))

cat(tot.sample, "of samples were used \n")
cat(tot.feature, "of full features\n")
cat(num.seed, "runs, each run contains", num.cv, "CVs.\n")

# if you think there are some unit run output is missing, use the following to figure out which ones are missing; then you can check out their log output to see whether there are specific errors

# you may need adjust posSeed or posSplit value
# Given a outcome name like 'glmnet_rdata_phenotype_seed1001_cv1.rdata', and the function splitting the output name by '_', the index of seed value is 4, and that of cv split is 5. So there are posSeed=4, posSplit = 5 below.
#checkOutput(path = with(runSpec, paste(project_home,'results', project_name, sep='/')), pattern = paste( '\\.rdata', sep=''), posSeed=4, posSplit = 5 )

```
run with `r runSpec$engine`.

## 1. Scores

```{r, fig.width=6, fig.height=6, echo=FALSE, message=FALSE, warning=FALSE}
plt <- plotHR(df.preval, Brier = T)
plt$overall +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Note for the **HR plot**: A HR value (per seed) is calculated by comparing the survival time between 'long' and 'short' survivors. These two group is defined by splitting samples based on _median_ predicted risk score; group_0 is predicted risk scores > median, which can be viewed as 'short survivors'. On the other hand, group_1 can be viewed as 'long survivors'. If the prediction is reasonable, the hazard ratio of group_1/group_0 should be < 1. The actual function used in calculating HR is ```coxph(Surv(time, status) ~ group.binary, df)```.


```{r, echo=FALSE, warning=FALSE}
knitr::kable(eval.minmax(plt$df.scores), digits = 3)
```


```{r, echo=FALSE}
err1 <- data.frame(plt$df.scores)
smoothScatter(err1[, 'cindex'], err1[,'BrierScore'], 
              main=paste('pearson corr:', round(cor(err1[, 'cindex'], err1[,'BrierScore']), 2) ))
lines(lowess(err1[, 'cindex'], err1[,'BrierScore']), col = 'green')
```


The following plot is to quickly see how well the prediction can separate long and short survivor. 
```{r, echo=FALSE}
size <- max(df.preval$size) ## default is total feature size. User can choose any RFE size
seed <- min(df.preval$seed) ## default chooses smaller seed.

df.in <- df.preval %>% 
  filter(size == size, seed==seed) %>% 
  mutate(pred.binary = ifelse(pred < median(pred), 1, 0))

fit1 <- survfit(Surv(col_surv, col_event) ~ pred.binary, data = df.in)
ggsurvplot(fit1, data = df.in, pval = TRUE)
```

## 2. Important Features

```{r, message=FALSE, echo=FALSE, warning=FALSE}
df.vimp.plt <- plotVIMP2(df.vimp, bin = 100, ignore_neg = T)
df <- df.vimp
## plot
df.vimp.plt$plt.dist.f
df.vimp.plt$plt.features

df.vimp.avg <- df.vimp.plt$df
write_csv(df.vimp.avg, paste0(runSpec$outP, "/Analyzed_FeatureImportance_avg.csv"))
```

```{r, fig.width=8, echo=FALSE}
library(pheatmap)
plotHeatmap(df.vimp.plt$df, df.orig)
```





